{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import math\n",
    "import sklearn\n",
    "import string\n",
    "import random\n",
    "from django.utils.crypto import get_random_string\n",
    "from sklearn import datasets\n",
    "import time\n",
    "import seaborn as sns\n",
    "plt.style.use('bmh')\n",
    "plt.rcParams[\"image.cmap\"] = \"winter\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement your own hash functions by scratch, no ready-made hash functions are allowed, you have to code them by hand. Read the class material and search the internet. The hash function will have to convert the string containing the password to a (potentially large) number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our hash function takes in input the string and a prime number (between 0 and 128)\n",
    "This hash function avoids equal hash codes for anagrams because uses the position of the letters in the string\n",
    "The prime numbers are chosen in a uniform way in the set of prime numbers between [0,128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set bloom filter dimension\n",
    "m=18*10**9\n",
    "\n",
    "#set prime numbers\n",
    "\n",
    "prime_numbers=[2,3,5,7,11,13,17,19,23,\n",
    "29,31,37,41,43,47,53,59,61,67\n",
    ",71,73,79,83,89,97,101,103,107,109\n",
    ",113,127]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b= np.zeros((m),dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_f(s,p):\n",
    "    A=0\n",
    "    for i in range(len(s)):\n",
    "        A=A+ord(s[i])*(p**i)\n",
    "    return A%m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use an array of 18 billions of lenght and 3 hash function \n",
    "For this reason the prime number are chosen in 9-step intervals from 127 to mantain uniformity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation of a list with the first 1000 passwords to check uniformity\n",
    "with open(\"passwords1.txt\", \"r\") as myfile:\n",
    "    list_of_passwords = [next(myfile) for x in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals=[]\n",
    "for s in list_of_passwords: \n",
    "    vals.append(hash_f(s,127))\n",
    "    vals.append(hash_f(s,79))\n",
    "    vals.append(hash_f(s,41))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([318., 278., 305., 311., 298., 311., 286., 305., 299., 289.]),\n",
       " array([5.09508200e+06, 1.80331724e+09, 3.60153939e+09, 5.39976155e+09,\n",
       "        7.19798371e+09, 8.99620586e+09, 1.07944280e+10, 1.25926502e+10,\n",
       "        1.43908723e+10, 1.61890945e+10, 1.79873166e+10]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot distribution of first 1000 password with 4 hash functions\n",
    "plt.hist(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Bloom filter implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "passwords1=open(\"passwords1.txt\", \"r\")\n",
    "passwords2=open(\"passwords2.txt\",\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BloomFilter(passwords1, passwords2):\n",
    "    start = time.time()\n",
    "    \n",
    "    #This step fills the bloom filter\n",
    "    for pw in passwords1:\n",
    "        b[hash_f(s,127)]=True\n",
    "        b[hash_f(s,79)]=True\n",
    "        b[hash_f(s,41)]=True\n",
    "        \n",
    "    #Find hits between passwords 2 and passwords 1 \n",
    "    count=0\n",
    "    for pw in passwords2:\n",
    "        if(bf[hash_f(pw,127)]==1 and bf[hash_f(pw,13)]==1 and bf[hash_f(pw,57)]==1):\n",
    "            count+=1\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=BloomFilter(passwords1, passwords2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of hash function used: ', 3)\n",
    "print('Number of duplicates detected: ', count)\n",
    "print('Probability of false positives: ', 0.000004515)\n",
    "print('Execution time: ', end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus part: can you provide exactly how many false positive do you have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify how many passwords of passwords2 are in passwords1 we implemented a function wich check for every password in passoword2 if it is in password1 (using in of Python) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_how_many(password1,password2):\n",
    "    how_many=0\n",
    "    for pw in password2:\n",
    "        if pw in password1:\n",
    "            how_many+=1\n",
    "    print('The number of passowords in passwords2 which are also in passowords1 is: ',how_many)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of passowords in passwords2 which are also in passowords1 is:  14000000\n"
     ]
    }
   ],
   "source": [
    "count_how_many(passwords1,passwords2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking the real duplicates number we can say that the number of false positive are 20.\n",
    "We are very close to the real result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Counting sort is a sorting algorithm that sorts the elements of an array by counting the number of occurrences of each unique element in the array. The count is stored in an auxiliary array and the sorting is done by mapping the count as an index of the auxiliary array.\n",
    "By watching this video you can understand how does the counting sort actualy works:\n",
    "    https://www.youtube.com/watch?v=TTnvXY82dtM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all the libraries we need in the counting, alphabetic and word sort:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import string\n",
    "import random\n",
    "import pandas as pd\n",
    "from django.utils.crypto import get_random_string\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have generated random numbers as input and sorted them with counting sort:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = []\n",
    "from random import randrange\n",
    "for i in range(20):\n",
    "    inp.append(randrange(1000))\n",
    "print(\"input is : \", inp) # printing the random input\n",
    "\n",
    "# geting the maximum value of the list and add 1 to it to create the frequency list and use the indexes\n",
    "mx = max(inp)\n",
    "m = mx+1 \n",
    "\n",
    "# generating the frequency list\n",
    "count = np.zeros(m,dtype=int) # making a zeros list with numpy\n",
    "for num in inp:\n",
    "    count[num] += 1 # adding 1 if the number exist and is equal to index of zeros list\n",
    "    \n",
    "sortedpos = []\n",
    "for i in range(len(count)):\n",
    "    # if the value in the zeros list is not zero add the index of that value to the new list as many time as the value\n",
    "    if count[i] != 0:\n",
    "        for j in range(count[i]):\n",
    "            sortedpos.append(i)\n",
    "print(\"sorted is : \", sortedpos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Result : now we can see the numbers are sorted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alphabetic and Word Sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here we have the a problem in alphabet and word sort (about lower and uppercase alphabets in each word), because we are using the ascii codes the counting sort will first sort the uppercase alphabet and then lowercase ones (ABCabc), so we have to change all the alphabet to lower case to become able to sort them in a right way (AaBbCc), but in the result we had to show the alphabet as they were in input (not all in lowercase), so here is the solution:\n",
    "\n",
    "We add a column to the the matrix of the words (which has ascii codes in) and put the order (initial index) of each word in it. Then we change all the characters to lowercase and convert them to ascii codes. Each time we sort them (in the way we will explain later) the input changes. It means that the order of the words in input will become like what the sorting function tell it to be. So we don't have to convert the ascii codes to character again! we just show the words with right order.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Sorting:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The idea is to sort the words based on the first character, so if the character does have any duplicate we will create a sublist for those duplicated characters resort the corresponding words based on the second character and it will continue till there is no duplicate left or the algorithm reaches the last character.\n",
    "\n",
    "For example: [Melika, manika, Dario, santiago, marco] for this case we first have the initial list of [m, m, d, s, m] and then we have the sublists of [m, m, m] , [d], [s] and we will move to the next character of those words related to the sublists that have duplicated characters in:\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                        m.               e\n",
    "                                        m.     -->       a.     -->     n\n",
    "                                        m.               a.             r \n",
    "                                        d\n",
    "                                        s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have generated a list with length of 'm' that contains random words with maximum len of 'n' as input,for generating alphabet (for second part of quesstion) it is just enough to put n = 2.\n",
    "then we convert them to numbers(ascii code of them) to become able to sort them with counting sort:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = []\n",
    "for m in range(10):\n",
    "    # returns a securely generated random string.\n",
    "    n = 8\n",
    "    # generating random words\n",
    "    inp.append(get_random_string(length = np.random.randint(1,n) , allowed_chars = (string.ascii_letters+\" \")))\n",
    "print(\"input is : \", inp) # printing the random input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the counting sort to sort the alphabet and words, it works exactly like the counting sort above\n",
    "def countingsort(sortedAlphabet):\n",
    "    sortedAlphabet = list(sortedAlphabet)\n",
    "    # geting the maximum value of the list of ascii codes of alphabet and add 1 to it to create the frequency list and use the indexes\n",
    "    mx = max(sortedAlphabet)\n",
    "    m = mx+1\n",
    "    order = [] # creating a list add the index of each number to it\n",
    "    # generating the frequency list\n",
    "    count = np.zeros(m,dtype=int) # making a zeros list with numpy\n",
    "    for num in sortedAlphabet:\n",
    "        count[num] += 1 # adding 1 if the number exist and is equal to index of zeros list\n",
    "\n",
    "    Alsortedpos = []\n",
    "    for i in range(len(count)):\n",
    "        # if the value in the zeros list is not zero add the index of that value to the new list as many time as the value\n",
    "        if count[i] != 0:\n",
    "            for j in range(count[i]):\n",
    "                Alsortedpos.append(i) # it adds the index of that value to the new list\n",
    "                # to be able to sort the words with their indexes we have to save them in a list\n",
    "                order.append(sortedAlphabet.index(i)) # it adds the index of each number to order\n",
    "                sortedAlphabet[sortedAlphabet.index(i)] = -1 # to avoid adding the same index in the next iteration\n",
    "    return Alsortedpos, order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsascii = []\n",
    "for word in inp:\n",
    "    # generating the matrix that have each word's character's ascii codes in it\n",
    "    max_len = max(map(len,inp)) + 1 # number of its columns should be the length of biggest word + 1 because of the order we added to the end of it \n",
    "    ordinals = np.zeros((len(inp), max_len), dtype = int)\n",
    "    # for each word we convert the characters to ascii codes and put them in seprated rows\n",
    "    cc = 0\n",
    "    for word in inp:\n",
    "        word = word.lower() #converting all the alphabet in each word to lowercase so we can sort in an apropriate way\n",
    "        ordinal = [ord(c) for c in word] #converting each character in each word to ascii code\n",
    "        ordinals[cc,0:len(ordinal)] = ordinal #put them in seprated rows\n",
    "        cc += 1\n",
    "    ordinals[:,-1] = range(len(inp)) # to keep the row positions of the words while swapping through the counting sort\n",
    "#print(ordinals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function checks the values and looks for duplicates and make sublists\n",
    "def sublist(sorted_arr,index):\n",
    "    s_list = []\n",
    "    l_list = [index[0]]\n",
    "    for i in range(1,len(sorted_arr)):\n",
    "        # to add the index of the value that has been duplicated to create sublist\n",
    "        if sorted_arr[i] == sorted_arr[i-1]:\n",
    "            l_list.append(index[i])\n",
    "        else:\n",
    "            # if there exist more than one duplicate we have to keep the indexes in another list \n",
    "            if len(l_list) > 1:\n",
    "                s_list.append(l_list)\n",
    "            l_list = [index[i]]\n",
    "    if len(l_list) > 1:\n",
    "        s_list.append(l_list)\n",
    "    return s_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this part we are making the sublists\n",
    "not_sorted_sublists = list([list(range(ordinals.shape[0]))]) # making the initial problem (the rows that still need to be sorted) that is sorting all the values in the first column\n",
    "for i in range(ordinals.shape[1]-1): # for range number of columns -1 because the last column contains the row positions\n",
    "    new_sublist = [] # to store the subproblems for the next column \n",
    "    for l in not_sorted_sublists:\n",
    "        sorted_arr, order = countingsort(ordinals[l,i]) #sorting those values related to the unsolved problem (the row that still need to be sorted)\n",
    "        ordinals[l,:] = ordinals[l,:][order,:] # reordering those rows in unsolved problem (the row that still need to be sorted). we are done with this problem\n",
    "        newest_sublist = sublist(sorted_arr, l) # generating new problems (the rows that still need to be sorted) if there exist duplicated values in sorted array\n",
    "        new_sublist = new_sublist + newest_sublist # each time addig the new ordered sublist to last ordered ones\n",
    "    not_sorted_sublists = new_sublist # replace new unsolved problems (the rows that still need to be sorted) with solved ones\n",
    "print(\"output is : \",[inp[i] for i in list(ordinals[:,-1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Result: Now we can see all the alphabets or words sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our step now has been studying time complexity of this code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every algorithm implemented here is based on Counting sort which has a O(n) complexity. \n",
    "So, for alphabet sort we have the simplest case of our algorithm (that is valid for alphabet sort and word sort) \n",
    "\n",
    "The alphabet sort can be traduced in a normal counting sort cause every letter is a numeric value. \n",
    "\n",
    "Now for the word sort as the simplest case we have a linear cost. The simplest case is when we have all the word starting with different letters: \n",
    "\n",
    "For example: \n",
    "\n",
    "['home', 'beer', 'dog', 'table]\n",
    "\n",
    "In this case the counting sort will be applied only on:\n",
    "\n",
    "['h', 'b', 'd', 't']\n",
    "\n",
    "So we will discover that the order of our words list is: \n",
    "\n",
    "['beer', 'dog', 'home', 'table']\n",
    "\n",
    "But in the worst case we will have all words having same letters except for the last one\n",
    "\n",
    "['coal', 'coax', 'coat']\n",
    "\n",
    "In this case the algorithm will compare n times m words so Counting sort will be computed m times (where m is number of words in the list and n is the max lenght of words in the list. In the example case every word has the same lenght). \n",
    "So we have O(mn) time complexity. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we want to show the time complexity of our code by using graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discussing time complexity empirically\n",
    "times = []\n",
    "for i in range(50,500,50):\n",
    "    inp = []\n",
    "    for j in range(i):\n",
    "        inp.append(get_random_string(length = np.random.randint(1,100) , allowed_chars = (string.ascii_letters+\" \")))\n",
    "\n",
    "    start_time = time.time()\n",
    "    def countingsort(sortedAlphabet):\n",
    "        sortedAlphabet = list(sortedAlphabet)\n",
    "        mx = max(sortedAlphabet)\n",
    "        m = mx+1\n",
    "        order = list()\n",
    "        count = np.zeros(m,dtype=int)\n",
    "        for num in sortedAlphabet:\n",
    "            count[num] += 1\n",
    "        Alsortedpos = []\n",
    "        for i in range(len(count)):\n",
    "            if count[i] != 0:\n",
    "                for j in range(count[i]):\n",
    "                    Alsortedpos.append(i)\n",
    "                    order.append(sortedAlphabet.index(i))\n",
    "                    sortedAlphabet[order[-1]] = -1\n",
    "        return Alsortedpos, order\n",
    "    wordsascii = []\n",
    "    for word in inp:\n",
    "        max_len = max(map(len,inp)) + 1\n",
    "        ordinals = np.zeros((len(inp), max_len), dtype = int)\n",
    "        cc = 0\n",
    "        for word in inp:\n",
    "            word = word.lower()\n",
    "            ordinal = [ord(c) for c in word]\n",
    "            ordinals[cc,0:len(ordinal)] = ordinal\n",
    "            cc += 1\n",
    "        ordinals[:,-1] = range(len(inp))\n",
    "\n",
    "    sublist_ = list([list(range(ordinals.shape[0]))])\n",
    "    for i in range(ordinals.shape[1]-1):\n",
    "        new_sublist = list()\n",
    "        for l in sublist_:\n",
    "            sorted_arr, order = countingsort(ordinals[l,i])\n",
    "            ordinals[l,:] = ordinals[l,:][order,:]\n",
    "            index = list(range(len(sorted_arr)))\n",
    "            sublist_l = sublist(sorted_arr, l)\n",
    "            new_sublist = new_sublist + sublist_l\n",
    "        sublist_ = new_sublist\n",
    "    [inp[i] for i in list(ordinals[:,-1])]\n",
    "    times.append(time.time()-start_time)\n",
    "#making plot \n",
    "plt.scatter(x = range(50,500,50), y = times)\n",
    "plt.xlabel('lenght of list')\n",
    "plt.ylabel('time(s)')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case showed in the graph we have time growing with lenght list.\n",
    "\n",
    "This shows a linear behaviour: \n",
    "\n",
    "Time grows with size list growing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining few functions needed for the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given 2 vectors this function returns their euclidean distance\n",
    "def dist (x, m):\n",
    "    diff = 0\n",
    "    # loop on all the components of the vectors, in this case our data is in a dataframe, so the lenght of one point is\n",
    "    # len(df.columns)\n",
    "    for i in range(len(df.columns)):\n",
    "        # do the difference between the i-th components and square it\n",
    "        diff += (x[i]-m[i])**2  \n",
    "    return (math.sqrt(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the center of mass given a cluster c\n",
    "def com (c):\n",
    "    xcm = []\n",
    "    # if the cluster doesn't have points in it we generate its center randomly again\n",
    "    ''''\n",
    "    Re-generate the center random if the cluster is of length zero it's a choice we made at the \n",
    "    beginning. Then we decided to leave the center as it is. The last option is implemented in \n",
    "    the function k_means. We left anyways the first option in this function even if it is never \n",
    "    satisfied.\n",
    "    '''\n",
    "    \n",
    "    if len(c) == 0:\n",
    "        for i in range(len(df.columns)):\n",
    "            xcm.append(np.random.uniform(low = mins[i], high = maxs[i]))\n",
    "    else:\n",
    "        # if the cluster is not empty we evaluate its center of mass\n",
    "        for i in range(len(df.columns)):\n",
    "            # for every component we calculate the center of mass on all points of the cluster\n",
    "            xcm.append(sum([row[i] for row in c])/len(c))\n",
    "    return xcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function\n",
    "def cost_function (centers, clusters):\n",
    "    cost = 0\n",
    "    # it is the sum of all distances squared between all centers and the respective \n",
    "    # cluster elements for all clusters\n",
    "    for i in range(k):\n",
    "        for j in range(len(clusters['cluster_{}'.format(i)])):\n",
    "            cost += dist(centers[i], clusters['cluster_{0}'.format(i)][j])**2\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate initial centroids\n",
    "def centroids ():\n",
    "    centers = []\n",
    "    for i in range(k):\n",
    "        row = []\n",
    "        # loop on all components \n",
    "        for i in range(len(df.columns)):\n",
    "            row.append(np.random.uniform(low = mins[i], high = maxs[i]))\n",
    "        # at the end we append the center in the centers list\n",
    "        centers.append(row)\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data from the dictionary to a dataframe\n",
    "def dicto_df (clusters):\n",
    "    clusters_df = {}\n",
    "    for i in range(len(df.columns)):\n",
    "        clusters_df[df.columns[i]] = []\n",
    "    clusters_df['cluster'] = []\n",
    "    # fill the dictionary taking into account also the cluster's tag\n",
    "    for i in range(k):\n",
    "        cl = clusters['cluster_{}'.format(i)]\n",
    "        for j in range(len(cl)):\n",
    "            for l in range(len(df.columns)):\n",
    "                clusters_df[df.columns[l]].append(cl[j][l])\n",
    "            clusters_df['cluster'].append(i+1)\n",
    "    # transform into a dataframe\n",
    "    return pd.DataFrame(clusters_df)\n",
    "\n",
    "# go back from standardized data to oridinal form\n",
    "def std_or(clusters_df, centers):\n",
    "    for i in range(0,len(clusters_df.columns)-1):\n",
    "        clusters_df.iloc[:, i] = clusters_df.iloc[:,i]*summary.iloc[2,i] + summary.iloc[1,i]\n",
    "    for i in range(len(centers)):\n",
    "        for j in range(len(centers[i])):\n",
    "            centers[i][j] = centers[i][j]*summary.iloc[2,j] + summary.iloc[1,j]\n",
    "    return (clusters_df, centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting 2 data features from the final dataframe\n",
    "# it takes as arguments the number of the 2 columns\n",
    "def scatterplot_clust (col1, col2):\n",
    "    x_c = []\n",
    "    y_c = []\n",
    "    for x in centers:\n",
    "        x_c.append(x[col1])\n",
    "        y_c.append(x[col2])\n",
    "    plt.scatter(clusters_df.iloc[:,col1], clusters_df.iloc[:,col2], c = clusters_df['cluster'],\n",
    "               s = 15)\n",
    "    plt.scatter(x_c, y_c, c = 'black', marker = 'D')\n",
    "    #plt.grid(color = 'white', linestyle = '--')\n",
    "    plt.xlabel(clusters_df.columns[col1])\n",
    "    plt.ylabel(clusters_df.columns[col2])\n",
    "\n",
    "def clust_his (col):\n",
    "    #sns.set(color_codes=True)\n",
    "    for i in range(k):\n",
    "        sns.distplot(clusters_df.iloc[:,col][clusters_df['cluster'] == i+1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import the data and standardize them. After applying the k-means algorithm the data are rescaled back to their original units using the function str_or()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data import \n",
    "df = pd.read_csv('wine.data', names = ['Cult', 'Alcohol', 'Malic Acid', 'Ash', 'Alcalinity of Ash', 'Magnesium', 'Total Phenols', \n",
    "                            'Flavonoids', 'Nonflavonoid Phenols', 'Proanthocyanins', 'Color Intensity','Hue', '% of Diluted wines','Proline' ])\n",
    "# for now we dont need this\n",
    "tags = df['Cult']\n",
    "del df['Cult']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the summary of the dataframe\n",
    "summary = df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the data\n",
    "for i in range(len(df.columns)):\n",
    "    df.iloc[:, i] = (df.iloc[:,i] - summary.iloc[1,i])/summary.iloc[2,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset explanation\n",
    "with open('wine.names', 'r') as f:\n",
    "    explanation = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Find max and min of each features, each element i of the 2 lists corresponts to the max and min \n",
    "element of the i-th column. This is neede to generate the centroids as explained in the next section.\n",
    "'''\n",
    "maxs = []\n",
    "mins = []\n",
    "for i in range(len(df.columns)):\n",
    "    maxs.append(max(df.iloc[:,i]))\n",
    "    mins.append(min(df.iloc[:,i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k - means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixed the numbers of clusters k we want to generate k random centers of the clusters. Each component of the center is bounded between the max and the min value of that feature. The number is generated from an uniform distribution. This last choice was made to optimize a little bit the algorithm, and to avoid bad initializations as shown in the 4-th question. \\\n",
    "For each center it is calculated the distance between each point of the dataset and the center, then that element is associated to the center of the cluster for which the distance is minimum. After having associated all points to the clusters we get the new centers of clusters as their center of masses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of clusters k is a global variable since it is used in almost every function. In k-means function the first argument is a boolean one, where 'True' is for generate centroids, 'False' if for not generate centroids. If the second option is  passed then it is possible to pass as second argument a matrix with the centroids. Basically the algorithm does 100 iterations, but it stops when the cost function, in the last 3 iterations, has not changed of a 1/10000 of its last value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means (cent_onoff, cent):\n",
    "    \n",
    "    centers = cent\n",
    "    # if the passed variable is True then the initial centroids are generated, otherwise they are passed\n",
    "    if cent_onoff == True:\n",
    "        centers = centroids()\n",
    "    # define the list that will contain the cost at each iteration\n",
    "    cost = []\n",
    "    # the algorithm will do a maximum of 100 iterations\n",
    "    for m in range(100):\n",
    "        # defining the clusters dictionary, it contains k values, associated with the k clusters\n",
    "        # each value is associated with a list that contains the points of that cluster        \n",
    "        clusters = {}\n",
    "        for i in range(k):\n",
    "            clusters['cluster_{}'.format(i)] = []\n",
    "\n",
    "        # loop on all points of our dataset\n",
    "        for i in range(len(df)):\n",
    "            # distXM[i] contains, for one point of the dataset x, the distance between it and the center i \n",
    "            distXM = []\n",
    "            x = list(df.loc[i])\n",
    "            # calculate the distance between x and all the centers\n",
    "            for i in range(k):\n",
    "                distXM.append(dist(centers[i], x))\n",
    "            # put that point to the cluster for which the distance with its center is minimum\n",
    "            for i in range(k):\n",
    "                if min(distXM) == distXM[i]:\n",
    "                    clusters['cluster_{}'.format(i)].append(x)\n",
    "         \n",
    "        # get the new centers of the clusters as their center of masses \n",
    "        for i in range(len(centers)):\n",
    "            # if the cluster does not have points, then its center does not change\n",
    "            if len(clusters['cluster_{}'.format(i)]) == 0:\n",
    "                centers[i] = centers[i]\n",
    "            else:\n",
    "                # if it contains points we evaluate the center of mass\n",
    "                centers[i] = com(clusters['cluster_{}'.format(i)])\n",
    "        # after having evaluated the centers we calculate the cost function        \n",
    "        cost.append(cost_function(centers, clusters))  \n",
    "        # early stopping\n",
    "        if len(cost) > 3:\n",
    "            # when the absolute value of the difference between the last 3 elements is minor than\n",
    "            # a 1/10000 the algorithm stops\n",
    "            if (abs(cost[-1]-cost[-2]) < cost[-1]/10000) & (abs(cost[-2]-cost[-3]) < cost[-1]/10000):\n",
    "                print('Converged in ',m,' iterations')\n",
    "                return (clusters, centers, cost)\n",
    "            \n",
    "        if m == 99:\n",
    "            print('The algorithm  did not converge in 100 iterations.')\n",
    "            return (clusters, centers, cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison between our kmeans and the sklearn one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the data in the format that is taken from sklearn, then set the same initial centroids for both algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3 # fix number of clusters at 3\n",
    "\n",
    "# sklearn kmeans\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "x = []\n",
    "for i in range(len(df)):\n",
    "    x.append(list(df.iloc[i]))\n",
    "center_in = centroids()# generate initial centroids\n",
    "\n",
    "clust = KMeans(n_clusters = k, init = np.array(center_in))\n",
    "clust.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our algorithm \n",
    "k = 3\n",
    "# use the same centroids as before\n",
    "clusters, centers, cost = k_means(False, center_in)\n",
    "clusters_df = dicto_df(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the centers from sklearn kmeans\n",
    "x_c = []\n",
    "y_c = []\n",
    "for x in clust.cluster_centers_:\n",
    "    x_c.append(x[6])\n",
    "    y_c.append(x[9])\n",
    "# comparison for 2 random features\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(x = df['Flavonoids'], y = df['Color Intensity'], c = clust.labels_, s = 15)\n",
    "plt.scatter(x_c,y_c, c = 'black', marker = 'D')\n",
    "plt.xlabel('Flavonoids')\n",
    "plt.ylabel('Color Intensity')\n",
    "plt.title('from scikit')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "scatterplot_clust(6, 9)\n",
    "plt.title('handmade')\n",
    "plt.show()\n",
    "# each color is a cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the same initial centroids we get almost the same results. In this case data are still standardized, just to show the results of the 2 algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost function and number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the cost in function of the iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cost)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iteration')\n",
    "plt.title('Cost function with k = {}'.format(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the cost for different values of k at a certain iteration. We execute the k-means for k from 1 to 10, then visualize it to choose the ideal value of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_tot = []\n",
    "for k in range(1,11):\n",
    "    cost_tot.append(k_means(False, centroids())[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets visualize the cost functions in function of k for different iteractions\n",
    "fig = plt.figure()\n",
    "n = np.arange(1, 11)\n",
    "plt.plot(n,[row[-1] for row in cost_tot])\n",
    "plt.xlabel('number of clusters k')\n",
    "plt.ylabel('cost')\n",
    "plt.title('k-means')\n",
    "ax = fig.add_subplot(111)\n",
    "ax.annotate('elbow', xy=(3, 1270),arrowprops=dict(facecolor='black', shrink=0.05), \n",
    "            xytext=(6, 1800),horizontalalignment='right', verticalalignment='top')\n",
    "plt.xticks(np.arange(min(n), max(n)+1, 1.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the elbow method a good number of clusters is 3: it may represent the 3 different cultivars or 3 tipologies of wines, red, white and rosé, or even 3 types of wines like Chardonnay, Pinot,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case: k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "clusters, centers, cost = k_means(True, [])\n",
    "clusters_df = dicto_df(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start visualizing the data with PCA. We are going to use sklearn to reduce the data dimensions from 13 to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "princomp = pca.fit_transform(clusters_df) \n",
    "pc_df = pd.DataFrame(data = princomp, columns = ['PC1', 'PC2'])\n",
    "plt.figure(figsize=(8,6))\n",
    "#plt.subplot(1,2,1)\n",
    "plt.scatter(pc_df['PC1'], pc_df['PC2'], c = clusters_df['cluster'], s = 15 )\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data have a 'nice' subdivision, we may think that 3 clusters is a good choice. We can verify if the 3 clusters represent the 3 different cultivars.\\\n",
    "Since we have the cultivar tags, we can confront that subdivision with our clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,6))\n",
    "plt.subplot(1,2,1)\n",
    "scatterplot_clust(6,9)\n",
    "plt.title('Labels are clusters')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(df['Flavonoids'], df['Color Intensity'], c = tags, s = 15)\n",
    "plt.xlabel('Flavonoids')\n",
    "plt.ylabel('Color Intensity')\n",
    "plt.title('Labels are cultivars')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__From the last result it is shown that, with high probability, the 3 clusters represent the 3 different cultivars.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check now what is our accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = 0\n",
    "for i in range(len(tags)):\n",
    "    if tags[i] != clusters_df['cluster'][i]:\n",
    "        cont += 1\n",
    "print ('Accuracy ', round(1-cont/len(tags),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check some features of the 3 clusters from their distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go back to original scale\n",
    "clusters_df, centers = std_or(clusters_df, centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each color is a cluster\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.subplot(2,2,1)\n",
    "clust_his (0)\n",
    "plt.subplot(2,2,2)\n",
    "clust_his(1)\n",
    "plt.subplot(2,2,3)\n",
    "clust_his (6)\n",
    "plt.subplot(2,2,4)\n",
    "clust_his(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of 2 features for the 3 clusters, each color is a cluster\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "scatterplot_clust(6, 10)\n",
    "plt.subplot(1,2,2)\n",
    "scatterplot_clust(6,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From various k-means tests, for both handmade and sklearn algorithms, were found some centroids initial conditions that do not converge to the optimal solution. In our algorithm each component of the centroids were generated between the maximum and minimum value of the feature associated to that component, but breaking this condition the algorithm can converge to solutions that are far away from the optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following case we are going to generate the centroids far away from all the other points, what will happen is that at the end there will be only 1 cluster, (associated to the nearest center of the 3 initial's), and the other 2 centers won't be associated to any other points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the centroids, but with the condition that each value must be 'far away' from the cluster points\n",
    "k = 3\n",
    "centers = []\n",
    "for i in range(k):\n",
    "    row = []\n",
    "    # loop on all components \n",
    "    for i in range(len(df.columns)):\n",
    "        row.append(np.random.uniform(low = 4*summary.iloc[2,i]*maxs[i], high = 5*summary.iloc[2,i]*maxs[i]))\n",
    "    # at the end we append the center in the centers list\n",
    "    centers.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters, centers, cost = k_means(False, centers)\n",
    "clusters_df = dicto_df(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(cost)\n",
    "plt.ylabel('cost')\n",
    "plt.subplot(1,2,2)\n",
    "scatterplot_clust(6, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from above the cost is at 2300, meanwhile the optimal is at around 1300, and only 1 cluster contains all the points. The cost will be higher than the optimal even just generating one center  far away from the others, there will be only 2 clusters at the end, and the fartest center will not contain any points. \\\n",
    "\\\n",
    "In general there can be many other bad initializations. This algorithm finds a local minimum, so any initial condition that is close to the local minimum will not let the algorithm converge to the optimal solution. For example setting certains seeds we  can generate random centroids that won't let the algorithm converge to the optimal solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrong initializations with seeds : 0, 5\n",
    "np.random.seed(5)\n",
    "k = 3\n",
    "centers = centroids()\n",
    "x_c = []\n",
    "y_c = []\n",
    "for x in clust.cluster_centers_:\n",
    "    x_c.append(x[6])\n",
    "    y_c.append(x[9])\n",
    "\n",
    "clusters, centers, cost = k_means(False, centers)\n",
    "clusters_df = dicto_df(clusters)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(cost)\n",
    "plt.ylabel('cost')\n",
    "plt.subplot(1,2,2)\n",
    "scatterplot_clust(6, 9)\n",
    "plt.scatter(x_c, y_c, c = 'red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous pictuers at the left we have the cost function, greater than the ideal 1300 cost, and on the right we have the final clustering. The red points represent the initial positions of the 'bad' centroids and the black points represent the centroids at which the algorithm converged. Probably the black ones represent a local minimum, and the red ones were 'too close' to those that they could not escape, like a ball trapped in a small hole (next figure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://miro.medium.com/max/984/1*7GQ5_OiJTVD7RmAj3ZlERA.png\",width=500, height=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
